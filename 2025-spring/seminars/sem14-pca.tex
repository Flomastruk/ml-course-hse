\documentclass[12pt,fleqn]{article}
% \usepackage{../lecture-notes/vkCourseML}
\usepackage{vkCourseML}
\hypersetup{unicode=true}
%\usepackage[a4paper]{geometry}
\usepackage[hyphenbreaks]{breakurl}
\usepackage{bbm}

\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{multirow}
\usepackage{tabularx}

\interfootnotelinepenalty=10000
\newcommand{\dx}[1]{\,\mathrm{d}#1} % маленький отступ и прямая d

\DeclareMathOperator{\Cov}{Cov}
%\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Corr}{Corr}
\def \E{\mathbb{E}}


\title{Машинное обучение\\Семинар 14\\Понижение размерности и метод главных компонент}
\author{}
\date{}

\begin{document}

\maketitle

В машинном обучении часто возникает задача уменьшения размерности признакового пространства. Для этого можно, например, удалять признаки, которые слабо коррелируют с целевой переменной; выбрасывать признаки по одному и проверять качество модели на тестовой выборке; перебирать случайные подмножества признаков в поисках лучших наборов.

Ещё одним из подходов к решению задачи является поиск новых признаков, каждый из которых является линейной комбинацией исходных признаков. В случае использования квадратичной функции ошибки при поиске такого приближения получается~\emph{метод главных компонент}~(principal component analysis, PCA), о котором и пойдет речь. 

Существует несколько разных постановок метода главных компонент. Мы разберём некоторые из них. 


\section{Максимизация дисперсии}

\begin{vkProblem} Рассмотрим многомерную случайную величину $(X_1, \ldots, X_D).$  Пусть у неё нулевое математическое ожидание. Мы собрали для такой случайной величины $\ell$ наблюдений и записали их в виде матрицы $X \in \RR^{\ell \times D}.$ Найдите выборочную ковариационную матрицу для выборки $X$.
\end{vkProblem}
\begin{esSolution}
Нам надо найти ковариационную матрицу между столбцами матрицы $X$. На её диагонали будут стоять дисперсии. Обычно $Var(X_j) = \E(X_j^2) - \E^2(X_j).$ Так как математическое ожидание каждого столбца равно нулю, тогда $Var(X_j) = \E(X_j^2),$ и оценку таких дисперсий можно найти по формуле

\[
\hat{\sigma}^2_j = \frac{1}{\ell} \sum_{i=1}^{\ell} x_{ij}^2.
\]

В остальных клетках матрицы будут стоять выборочные ковариации. Обычно $\Cov(X_j, X_k) = \E(X_j X_k) - \E(X_j) \E(X_k).$ Так как математическое ожидание каждого столбца равно нулю, тогда $\Cov(X_j, X_k) = \E(X_j X_k),$ и их оценки можно найти по формуле

\[
\hat{\rho}_{jk} = \frac{1}{\ell} \sum_{i=1}^{\ell} x_{ij}\cdot x_{ik}.
\]

Получается, что выборочную ковариационную матрицу можно записать как~$\frac{X^TX}{\ell}.$ В вычислениях ниже нам будет часто встречаться матрица $X^TX.$ Будем держать в голове, что она пропорциональна выборочной ковариационной матрице. Отметим, что если бы математическое ожидание было бы неизвестно, для получения несмещённой оценки дисперсии и ковариации мы должны были бы делить на $\ell - 1.$
\end{esSolution}

Пусть~$X \in \RR^{\ell \times D}$~--- матрица <<объекты-признаки>>, где~$\ell$~--- число объектов, а~$D$~--- число признаков. Будем считать, что данные являются центрированными~--- то есть среднее в каждом столбце матрицы~$X$ равно нулю.  Мы хотим уменьшить размерность пространства до~$d$. При этом каждый новый признак будет линейно выражаться через исходные с какими-то весами

$$
z_{ij} = \sum_{k = 1}^{D} x_{ik} u_{kj}.
$$

Чем больше дисперсии выборки мы сохранили, тем больше сохраняется информации. Будем искать веса~$u_1, \dots, u_D \in \RR^D$ так, чтобы сохранить максимальную дисперсию: 

\begin{enumerate}
\item  $z_1 = X u_1:$ выборочная дисперсия $z_1$ должна быть максимальной при условии $\|u_1\|^2 = 1$.

\item $z_2 = X u_2:$ выборочная дисперсия $z_2$ должна быть максимальной при условии $\|u_2\|^2 = 1$ и $u_1 \perp u_2.$

\item $z_3 = X u_3:$ выборочная дисперсия $z_3$ должна быть максимальной при условии $\|u_3\|^2 = 1$ и $u_1 \perp u_3, u_2 \perp u_3$

\item и так далее
\end{enumerate}

В матричном виде мы можем записать это как $Z = XU$. При этом матрица $U$ будет ортогональной, то есть $U^TU = I$. Нам это нужно, чтобы задача оптимизации имела единственное решение. 

Давайте попробуем найти первую главную компоненту. Сведём все требования к ней в оптимизационную задачу. Данные нормированы. Преобразование предполагает, что мы взвесим исходные случайные величины с какими-то весами. Получается, что математическое ожидание по-прежнему будет нулевым, и выборочная дисперсия будет совпадать со вторым моментом

\[
    \begin{cases}
        \| X u_1 \|^2 = u_1^T X^TX u_1 \to \max_{u_1} \\
        \|u_1\|^2 = u_1^T u_1 = 1.
    \end{cases}
\]

Запишем лагранжиан (знак минус перед $\lambda$ используется для удобства):
\[
    L(u_1, \lambda)
    =
    u_1^T X^TX u_1 - \lambda (u_1^T u_1 - 1).
\]
Продифференцируем его и приравняем нулю:
\[
    \frac{\partial L}{\partial u_1}
    =
    2 X^T Xu_1 - 2 \lambda u_1
    =
    0.
\]
Получается, что 
\[
    X^T Xu_1 = \lambda u_1
\]


Отсюда получаем, что~$u_1$ должен быть собственным вектором матрицы~$X^T X$.
Учтём это и преобразуем функционал:
\[
    \| X u_1 \|^2
    =
    u_1^T X^T X u_1
    =
    \lambda u_1^T u_1
    =
    \lambda
    \to
    \max_{u_1}
\]
Значит, собственный вектор~$u_1$ должен соответствовать максимальному
собственному значению матрицы $X^TX.$ Напомним, что эта матрица пропорциональна ковариационной матрице. То есть именно она характеризует дисперсию выборки. 

Для следующих компонент к оптимизационной задаче будут добавляться требования
ортогональности предыдущим компонентам.
Решая эти задачи, мы получим, что главная компонента~$u_i$
равна собственному вектору, соответствующему~$i$-му собственному значению.

После того как найдены главные компоненты, можно проецировать на них и новые данные.
Если нам нужно работать с тестовой выборкой~$X^{\text{test}}$, то её проекции вычисляются как $\underset{m \times d}{Z^{\text{test}}} = \underset{m \times D}{X^{\text{test}}} \underset{D \times d}{U}$.


\begin{vkProblem} Давайте убедимся, что мы поняли метод главных компонент и попробуем ответить на следующие вопросы: 
    \begin{enumerate} 
        \item Чему будет равно среднее $z_1$, если исходные данные центрированы? А $z_2$?
        \item Докажите, что из того, что $u_1 \perp u_2$ следует, что $z_1 \perp z_2$. Чему будет равна $\Cov(z_1, z_2)$?
        \item Докажите, что $Var(x_1) + \ldots + Var(x_D) = Var(z_1) + \ldots + Var(z_D)$
        \item Выразите $\sum_{k=1}^D \|z_k\|^2$ через собственные числа. Как с помощью собственных чисел оценить долю объяснённой компонентами дисперсии?  
    \end{enumerate}
\end{vkProblem}
\begin{esSolution}
    \begin{enumerate} 
        \item Все исходные переменные центрированы, их математическое ожидание равно нулю. Мы взвешиваем случайные величины с какими-то весами. Математическое ожидание от этого никак не меняется. Оно остаётся нулевым. 
        
        \item Из ортогональности $u_1$ и $u_2$ следует ортогональность $z_1$ и $z_2$
        \[
        z_1^T z_2 = u_1^TX^TXu_2 = \lambda_2 \cdot u_1^T u_2 = 0.
        \]
        
        Тут мы воспользовались тем, что $u_2$ это собственный вектор матрицы $X^TX$. Выходит, что 
        \[
        \Cov(z_j, z_k) = \E(z_j \cdot z_k) - \E(z_j) \cdot \E(z_k) = \E(z_j \cdot z_k) = z_j^T z_k = 0.
        \]
        
        \item С одной стороны \[Var(x_1) + \ldots + Var(x_D) = \frac{1}{\ell -1}\Tr(X^TX),\] 
        
        так как все дисперсии в ковариационной матрице находятся на диагонали, а след представляет из себя сумму диагональных элементов. 
        
        С другой стороны  \[Var(z_1) + \ldots + Var(z_D) = \frac{1}{\ell - 1}\Tr(Z^TZ) = \frac{1}{\ell - 1}\Tr(U^TX^TXU) = \frac{1}{\ell - 1}\Tr(X^TX),\]
        
        так как $U^TU = I$, под следом матрицы можно переставлять по циклу и, снова, все дисперсии находятся в матрице $Z^TZ$ на диагонали. Обратите внимание, что все недиагональные элементы в $Z^TZ$ будут нулевыми. Мы доказали это в предыдущем пункте. 
        
        \item Так как все математические ожидания нулевые, $\|z_k\|^2 = z_k^Tz_k = Var(z_k).$ Выходит, что $\sum_{k=1}^D \|z_k\|^2 = \sum_{k=1}^D Var(z_k) = \sum_{k=1}^D Var(x_k).$ Выразим эту сумму через собственные значения
        
        \[\sum_{k=1}^D \|z_k\|^2 = \sum_{k=1}^D \|Xu_k\|^2 = \sum_{k=1}^D u_k^T X^T X u_k = \sum_{k=1}^D u_k^T \lambda_k u_k = \sum_{k=1}^D \lambda_k.\]
        
        Выходит, что если мы оставляем в данных $d$ компонент, то дробь $\frac{\lambda_1 + \ldots + \lambda_d}{\lambda_1 + \ldots + \lambda_D}$ показывает, какая доля дисперсии сохранилась после проецирования выборки на главные компоненты. 
     \end{enumerate}
\end{esSolution}


\begin{vkProblem} 
    Фил придумал метод бесполезных компонент. Как и в методе главных компонент, бесполезные компоненты являются линейными комбинациями исходных переменных. Бесполезные компоненты также ортогональны между собой. Вектор весов, с которыми исходные переменные входят в бесполезную компоненту, всегда имеет единичную длину. В отличие от метода главных компонент, первая бесполезная компонента обладает наименьшей выборочной дисперсией. Вторая бесполезная компонента ортогональна первой и обладает наименьшей выборочной дисперсией при условии ортогональности. И так далее. 
    
    Как связаны метод бесполезных компонент и метод главных компонент?
\end{vkProblem}
\begin{esSolution}
Компоненты из метода бесполезных компонент --- это ровно главные компоненты, но только перечисленные в обратном порядке. 
\end{esSolution}


\section{Минимизация ошибки приближения}

Пусть~$X \in \RR^{\ell \times D}$~--- матрица <<объекты-признаки>>, где~$\ell$~--- число объектов, а~$D$~--- число признаков. Поставим задачу уменьшить размерность пространства до~$d$. Будем считать, что данные являются центрированными~--- то есть среднее в каждом столбце матрицы~$X$ равно нулю.

Мы хотим, чтобы каждый новый признак линейно выражался через исходные

$$
z_{ij} = \sum_{k = 1}^{D} x_{ik} u_{kj}.
$$

В матричном виде это можно записать как 

$$
Z = XU^T.
$$

Чтобы у этого уравнения существовало единственное решение, нужно ввести на матрицу весов ограничения. Пусть она будет ортогональной $U^T U = I$. Если это требование выполнено, можно получить формулу для матрицы $X$

$$
X = ZU.
$$

В матрице $Z$ будет меньше признаков, чем в $X$, так как $d < D.$ Это равенство нельзя выполнить строго. Нам придётся потребовать, чтобы отклонение матрицы признаков $X$ от $ZU$ было как можно меньше. Размер этого отклонения будем вычислять с помощью нормы Фробениуса: 

$$
\begin{cases} 
||X - ZU||_F \to \min_{Z,U} \\
U^TU = I
\end{cases} 
$$

Норма Фробениуса матрицы — это сумма квадратов ее значений. Получившаяся задача  --- это задача матричного разложения. Необходимо представить матрицу $X$ в виде
произведения двух матриц $U$ и $W$, которые будут иметь меньший ранг. То есть нужно уменьшить ранг матрицы, при этом потеряв как можно меньше информации в ней.

Решением данной задачи также являются собственные векторы ковариационной матрицы.


\section{Задачи на метод главных компонент\footnote{Задачи взяты из коллекции Бориса Демешева: \url{https://github.com/bdemeshev/mlearn_pro}}}


\begin{vkProblem} 
У бесстрашного исследователя Ильдуса есть набор точек $(x_1, y_1)$, \ldots, $(x_n, y_n)$. Ильдус находит прямую, сумма расстояний от которой до каждой точки минимальна. Верно ли, что прямая проходит через точку $(\bar x, \bar y)$?
\end{vkProblem}
\begin{esSolution}
Да. Например, прямую можно задать вектором $w$ единичной длины и числом $b$,
$w^T \begin{pmatrix}
x \\
y \\
\end{pmatrix} = b$.
Обозначим вектор $(x_i, y_i)^T$ буквой $v_i$.
Тогда нам нужно минимизировать функцию
\[
\sum (w^T v_i - b)^2.
\]

Дифференциируем по $b$ и получаем $\sum w^T v_i = n\cdot b$,
что и означает, что прямая проходит через среднюю точку,
\[
w^T \begin{pmatrix}
\bar x \\
\bar y \\
\end{pmatrix} = b.
\]
\end{esSolution}


\begin{vkProblem} 
Есть две переменных, $x_1 = (1, 0, 0, 3)^T$, $x_2 = (3, 2, 0, 3)^T$. Найдите первую и вторую главные компоненты.
\end{vkProblem}
\begin{esSolution}
Матрица с центрированными столбцами имеет вид: $\widetilde{X} = \begin{pmatrix}
0 & 1 \\
-1 & 0 \\
-1 & -2 \\
2 & 1 \\
\end{pmatrix}$

Тогда $\widetilde{X}^T\widetilde{X} =
\begin{pmatrix}
6 & 4 \\
4 & 6 \\
\end{pmatrix}$.

Её собственные числа: $\lambda_1 = 10$, $\lambda_2 = 2$, собственные вектора $u_1 = (1/\sqrt{2} \quad 1/\sqrt{2})^T$, $u_2 =  (1/\sqrt{2} \quad -1/\sqrt{2})$.
Найдём главные компоненты:
\[
Z = XU =
\begin{pmatrix}
0 & 1 \\
-1 & 0 \\
-1 & -2 \\
2 & 1 \\
\end{pmatrix}
\begin{pmatrix}
1/\sqrt{2} & 1/\sqrt{2} \\
1/\sqrt{2} & -1/\sqrt{2} \\
\end{pmatrix} =
\begin{pmatrix}
1/\sqrt{2} & -1/\sqrt{2} \\
-1/\sqrt{2} & -1/\sqrt{2} \\
-3/\sqrt{2} & 1/\sqrt{2} \\
3/\sqrt{2} & 1/\sqrt{2}\\
\end{pmatrix}
\]
Первая и вторая главные компоненты — это первый и второй столбцы матрицы $Z$ соответственно.
\end{esSolution}




% \begin{vkProblem} 
%     Найдите прямую, у которой сумма квадратов расстояний до точек $(0,0)$, $(1, 1)$, $(2, 1)$ будет минимальной. Чему равна при этом доля объяснённого разброса точек?
% \end{vkProblem}
% \begin{esSolution}
%   omg
% \end{esSolution}



% \begin{vkProblem} 
%     Пионеры, Крокодил Гена и Чебурашка собирали металлолом несколько дней подряд. В распоряжение иностранной шпионки, гражданки Шапокляк, попали ежедневные данные по количеству собранного металлолома: вектор $g$ — для Крокодила Гены, вектор $h$ — для Чебурашки и вектор $x$ — для Пионеров. Гена и Чебурашка собирали вместе, поэтому выборочная корреляция между $g$ и $h$ равна $-0.9$. Гена и Чебурашка собирали независимо от Пионеров, поэтому выборочные корреляции $g,h$ и $x$ равны нулю. Если регрессоры $g$, $h$ и $x$ центрировать и нормировать, то получится матрица $\tilde{X}$.
%     \begin{enumerate}
%         \item Вычислите одну или две главные компоненты (выразите их через вектор-столбцы матрицы. $\tilde{X}$), объясняющие не менее $70\%$ общей выборочной дисперсии регрессоров.
        
%         \item Шпионка Шапокляк пытается смоделировать ежедневный выпуск танков, $y$. Выразите оценки коэффициентов регрессии $y = w_0 + w_1 g +w_2 h +w_3 x$ через оценки коэффициентов регрессии на главные компоненты, $y = v_0 + v_1 z_1 + v_2 z_2$, объясняющие не менее $70\%$ общей выборочной дисперсии.
%     \end{enumerate} 
% \end{vkProblem}
% \begin{esSolution}
%     Все дисперсии у нас единичные. Корреляции совпадают с ковариациями. Получается, что ковариационная матрица выглядит как  
%     \[
%     \frac{(\tilde{X}^T\tilde{X})}{\ell - 1} = \begin{pmatrix}  1 & -0.9 & 0 \\ -0.9 & 1 & 0 \\ 0 & 0 & 1 \end{pmatrix}.
%     \]
    
%     Чисто формально для PCA нам надо искать собственные число и вектора матрицы $(\tilde{X}^T\tilde{X})$. Когда мы выбираем для собственного числа какой-то конкретный собственный вектор, мы приводим его к единичной длине. Если мы подберём собственные вектора для матрицы 
    
    
%     Собственными числами матрицы $\frac{(\tilde{X}^T\tilde{X})}{\ell - 1}$ будут  $\lambda_1 = 1.9, \lambda_2 = 1, \lambda_3 = 0.1.$ Чтобы покрыть больше $70\%$ дисперсии нужно взять первые две компоненты, так как $\frac{1.9}{1.9 + 1 + 0.1} < 0.7,$ а $\frac{1.9 + 1}{1.9 + 1 + 0.1} > 0.7$. 
    
%     Собственными векторами окажутся $\left(-1/\sqrt{2}, 1/\sqrt{2}, 0 \right)^T, \left(0, 0, 1\right)^T, \left(1/\sqrt{2}, 1/\sqrt{2}, 0\right)^T.$ Выходит, что 
    
%     \[
%     \begin{aligned}
%         & z_1 = -\frac{1}{\sqrt{2}} \cdot g + \frac{1}{\sqrt{2}} \cdot h \\
%         & z_2 = x. \\
%     \end{aligned}
%     \]
    
    
    
    
%     \end{enumerate} 
% \end{esSolution}


\section{Ядровой переход в методе главных компонент}

Если внимательно посмотреть на обычный PCA, можно заметить, что, преобразование данных $X=ZU$ линейно. Как известно, признаковое пространство может быть устроено куда сложнее, зависимости могут быть и нелинейными. В методе главных компонент это выльется в то, что ошибка приближения никогда не упадет ниже какого-то числа -- потери дисперсии будут в любом случае, любая ось в исходном евклидовом пространстве потеряет часть дисперсии при проекции на нее.

Для каких-то задач легко придумать трансформацию данных, которая упростит задачу. Понять, какой на самом деле размерности задача, бывает непросто, но в ряде случаев хватает картинки. Посмотрим на примеры ниже

\pagebreak

\begin{figure}
\centering
\begin{minipage}{0.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{blob.png}
  % \caption{A subfigure}
  \label{fig:sub1}
\end{minipage}%
\begin{minipage}{0.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{circles.png}
  % \caption{A subfigure}
  \label{fig:sub2}
\end{minipage}
\caption{Примеры главных осей}
\label{fig:test}
\end{figure}

Если в первом случае главные оси выделяются хорошо, к тому же дисперсия явно больше распределена вдоль одной из них, то на второй картинке оси равнозначны -- при любой проекции потеряем примерно половину информации. Истинная размерность окружности равна 1. Если перейти в полярные координаты, достаточно одной координаты (радиуса), чтобы задать ее. Либо можно добиться того же, добавив признаки вида $x_i^2$, чья сумма даст квадрат радиуса. Конечно, заранее нужная трансформация неизвестна, вместо этого можно использовать знакомый нам ядровой переход.

Как и прежде, для этого понадобится функция $\phi: \mathbb{X}~\to~H$, которая переводит исходные признаки в новое спрямляющее пространство, и само ядро $K(x, z) = x^Tz$. Также напомним, что новое признаковое описание будем обозначать $\Phi$, а за $\text{K}=\Phi\Phi^T$ возьмем матрицу Грама в спрямляющем пространстве. Следующие предположения аналогичны рассуждениям для обычного PCA 

\begin{enumerate}
\item Данные должны быть центрированы, $\sum_{i=0}^D \phi(x_i) = 0$. \\ После ядрового перехода нет никакой гарантии, что среднее останется равным 0. Как обычно, это можно сделать руками, если вычесть среднее. А дальше, путем пары алгебраических преобразований, можно получить выражения для центрированной матрицы $\hat{\text{K}}$
\begin{equation}
  \begin{array}{l}
    \hat{\phi}(x) = \phi(x) - \frac{1}{D}\sum_{j=1}^D\phi(x_j) \\
    \hat{K}(x, z) = \hat{\phi}(x)^T\hat{\phi}(z) \\
    \hat{K}(x, z) = K(x, z) - \frac{1}{D}\sum_{n=1}^D (K(x, x_n) + K(x_n, z)) + \frac{1}{D^2}\sum_{n, m=1}^DK(x_n, x_m) \\
    \hat{\text{K}} = \text{K} - 2\cdot\mathbbm{1}_{1/D} \text{K} + \mathbbm{1}_{1/D} \cdot \text{K} \cdot \mathbbm{1}_{1/D}
  \end{array}
\end{equation}
Матрица $\mathbbm{1}_{a}$ это матрица, на всех элементах которой стоит $a$. Матрица $\text{K}$ пригодится для дальнейших махинаций

\break

\item Хочется, чтобы уменьшение дисперсии в новом пространстве было как можно меньше. \\ 
Будем исходить из того же предположения, что и в исходном PCA -- главные компоненты можно найти при помощи собственных векторов матрицы ковариации в новом пространстве

Ковариационная матрица в $H$ имеет вид 
$$
C = \frac{1}{D}\sum_{i=0}^D \phi(x_i) \phi(x_i)^T = \frac{1}{D}\Phi^T\Phi
$$

Понятно, что найти ее в явном виде можно не всегда, матрица $\Phi$ может быть бесконечномерной, но это и не требуется. Заметим, что $\phi(x_i)^T v$ это скаляр, для любого вектора $v$, а значит

$$
Cv = \lambda v
$$
$$
    Cv = \frac{1}{D}\sum_{i=0}^D \phi(x_i) \left(\phi(x_i)^T v \right)= \frac{1}{D}\sum_{i=0}^D \phi(x_i) a_i = \lambda v
$$

Отсюда следует, что любой собственный вектор матрицы $C$ будет представим в виде линейной комбинации в новом пространстве, а значит главные компоненты можно найти по формуле, зная ядро $K$ и векторы $a$
\begin{equation}
  \begin{array}{l}
    y = \phi(x_i)^T u = \sum_{j=1}^d a_{ji} \phi(x_i)^T \phi(x_j) = \sum_{j=1}^d a_j K(x_i, x_j)
  \end{array}
\end{equation}

Здесь же можно сразу отрегулировать, сколько главных компонент хочется оставить -- достаточно просуммировать до индекса $d$ -- новой уменьшенной размерности пространства

\item Оказывается, чтобы найти векторы $a$ выше, достаточно посчитать собственные значения матрицы $\text{K}$

$$
Cv = \frac{1}{D}\sum_{i=0}^D \phi(x_i) \phi(x_i)^T v = \frac{1}{D}\sum_{i=0}^D \phi(x_i) \phi(x_i)^T \sum_{j=0}^D a_j \phi(x_j) = \frac{1}{D} \Phi^T\Phi\Phi^Ta
$$
$$
\lambda v = \lambda \sum_{j=0}^D a_j \phi(x_j) = \lambda \Phi^Ta
$$
$$
\Phi^T(\Phi\Phi^Ta - D\lambda a) = 0
$$

Сделаем одно допущение -- будем считать, что новое представление данных $\Phi$ не будет нулевым. Иначе ядровой переход кажется бессмысленным. В таком случае, можно избавиться от $\Phi^T$ и перейти к выражению
\begin{equation}
  \begin{array}{l}
    \text{K}a = D\lambda a
  \end{array}
\end{equation}

Таким образом, необходимости считать ковариационную матрицу напрямую нет, для уменьшения размерности хватит информации про само ядро.

Теперь обсудим, как применять KernelPCA. Во-первых, обратим внимание, что $\text{A}$ строится по объектам через $\Phi\Phi^T$, то есть содержит $n$ строк. Нулевые собственные значения так или иначе не интересны для проекции, поэтому брать размерность векторов больше, чем $D$ не имеет смысла. В обычном PCA $U$ строилась по признакам, т.е. $X^TX$, где будет не более $D$ строк. 

Затем вспомним, что из сингулярного разложения следует, что $Z = XU = V\Sigma$. Поскольку теперь напрямую известно $V$, оно же $\text{A}$ -- собственные векторы $\text{K}$, можем этим воспользоваться, чтобы найти трансформацию в матричном виде. Пусть $n$ -- размерность обучающей выборки, а $n$ -- тестовой, тогда
\begin{equation}
  \begin{array}{l} 
    z = \sigma_i a_i = \sqrt{\lambda_i}a_i = \frac{1}{\sqrt{\lambda_i}}\text{K} a_i\\
    \text{K}^{\text{test}} = \left(K(x_j^{\text{test}}, x_i^{\text{train}} \right)_{i=1, j=1}^{m, n} \\
    \Lambda^{-\frac{1}{2}} = diag(\sqrt{\lambda_i})_i \\
    \underset{m \times d}{Z^{\text{test}}} = \underset{m \times n}{\text{K}^{\text{test}}} \ \underset{n \times d}{A} \ \underset{d\times d}{\Lambda^{-\frac{1}{2}}}
  \end{array}
\end{equation}


\item Векторы главных осей $u$ в новом признаковом пространстве должны быть единичной длины  \\ Как ни странно, для этого тоже достаточно знать лишь матрицу $\text{K}$ и векторы $a$
\begin{equation}
  \begin{array}{l}
  u^Tu = 1 \implies \sum_{i, j} a_i a_j \phi(x_i)^T \phi(x_j) = a^T\text{K}a = 1
  \end{array}
\end{equation}
\end{enumerate}

Таким образом, итоговый алгоритм очень похож на исходный PCA за парой деталей, которые были обозначены выше. Фактически это применение трансформации для нахождения нового признакового пространства и одновременно понижение размерности. Подведем итог и распишем алгоритм для двух случаев PCA

\begin{center}
\begin{tabularx}{\linewidth}{|c|X|X|} 
  \hline
 \textbf{Этап} & \textbf{PCA} & \textbf{KernelPCA} \\ 
 \hline
 \multirow{3}{*}{Обучение}
 &
 Работаем с исходным пространством $\mathbb{X}$
 &
 Задаем ядро $K(x, z)$, находим $\Phi$ (явно или, например, при помощи RFF)
 \\ 
 \cline{2-3}
 & 
 Центрируем пространство $\mathbb{X}$
 & 
 Центрируем и находим $\hat{\text{K}}$
 \\
 \cline{2-3}
 &
 Cоставляем матрицу $U_d$ размера $D \times d$ из с.в. $X^TX$
  &
 Cоставляем матрицу $\text{A}_d$ размера $\ell \times d$ из с.в. $\hat{\text{K}}$ и матрицу с.з. $\Lambda_d$
 \\
 \hline
  \multirow{2}{*}{Применение}
  &
  Центрируем $X^{\text{test}}$
  &
  Находим центрированную $\hat{\text{K}}^{\text{test}}$
  \\
\cline{2-3}
 &
 Ищем $Z^{\text{test}} = X^{\text{test}}U_d$ 
 &
 Ищем $Z^{\text{test}} = \hat{\text{K}}^{\text{test}}\text{A}_d\Lambda_d^{-\frac{1}{2}}$
 \\
\hline
\end{tabularx}
\end{center}

Главные преимущества ядерного метода главных компонент:
\begin{itemize}
    \item в новом спрямляющем пространстве может сохраниться больше дисперсии, из-за чего новые представления будут более осмысленными
    \item не нужно придумывать трансформацию, которая сохранит максимальное количество дисперсии
    \item можно уловить нелинейные зависимости и, например, превратить неразделимую задачу классификации в разделимую, либо же кластеризовать точки, между которыми нет линейной связи, пример ниже
\end{itemize}

\begin{figure}[h|t]
\centering
\begin{minipage}{0.36\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{sphere.png}
  % \caption{PCA}
  \label{fig:sub1}
\end{minipage}%
\begin{minipage}{0.36\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{sphere_kernel.png}
  % \caption{KernelPCA}
  \label{fig:sub2}
\end{minipage}
\caption{Результат PCA и KernelPCA на трехмерной сфере}
\label{fig:test}
\end{figure}

Его минусы следующие:
\begin{itemize}
    \item ядро нужно выбрать
    \item у этого метода больше гиперпараметров, чем у обычного варианта, что усугубляется тем, что единой метрики качества понижения размерности нет (ошибка приближения будет считаться уже в $H$, не получится сравнить с ошибкой в исходном пространстве)
    \item при большом объеме данных ядро считается достаточно долго
    \item теряется интерпретируемость -- главные компоненты более не соответствуют исходным признакам
    \item нет аналитического обратного преобразования из $H$ (но можно посчитать численно)
\end{itemize}

\end{document}
